{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indoor Navigation ML Decision Models\n",
    "\n",
    "This notebook demonstrates the full pipeline:\n",
    "1. Loading annotated data\n",
    "2. Extracting features\n",
    "3. Training StepFilter and AnchorSelection models\n",
    "4. Evaluating performance\n",
    "5. Running inference on new routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print('Project root:', PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Training Data\n",
    "\n",
    "Load human annotations from local submissions or Google Sheets,\n",
    "then re-run route generation to capture numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.data.data_loader import build_training_dataset\n",
    "from ml.data.dataset import train_val_split, save_dataset_cache, load_dataset_cache\n",
    "\n",
    "CACHE_PATH = 'ml/checkpoints/dataset_cache.json'\n",
    "\n",
    "# Load from local submissions (or use source='sheets', sheets_url='...')\n",
    "if os.path.exists(CACHE_PATH):\n",
    "    records = load_dataset_cache(CACHE_PATH)\n",
    "else:\n",
    "    records = build_training_dataset(source='local', venue='zorlu')\n",
    "    if records:\n",
    "        save_dataset_cache(records, CACHE_PATH)\n",
    "\n",
    "print(f'Total route records: {len(records)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inspect Features\n",
    "\n",
    "Extract feature vectors from a single route to understand dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.features.feature_extractor import FeatureExtractor\n",
    "\n",
    "fe = FeatureExtractor()\n",
    "\n",
    "if records:\n",
    "    rec = records[0]\n",
    "    step_feats = fe.extract_step_features(rec)\n",
    "    anchor_feats = fe.extract_anchor_features(rec)\n",
    "\n",
    "    print(f'Route: {rec[\"route_id\"]}')\n",
    "    print(f'Step features shape: {step_feats.shape}')  # (N_steps, 13)\n",
    "    print(f'Anchor feature records: {len(anchor_feats)}')\n",
    "    if anchor_feats:\n",
    "        print(f'Per-anchor feature shape: {anchor_feats[0][\"features\"].shape}')  # (3, 8)\n",
    "    print()\n",
    "    print('Step labels:', rec.get('step_labels', {}))\n",
    "    print('Anchor labels:', rec.get('anchor_labels', {}))\n",
    "else:\n",
    "    print('No records available. Add data to submissions/ folder.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.data.dataset import StepFilterDataset, AnchorSelectionDataset\n",
    "\n",
    "train_recs, val_recs = train_val_split(records, val_ratio=0.2, seed=42)\n",
    "print(f'Train: {len(train_recs)} routes, Val: {len(val_recs)} routes')\n",
    "\n",
    "train_sf = StepFilterDataset(train_recs, fe)\n",
    "val_sf = StepFilterDataset(val_recs, fe)\n",
    "print(f'\\nStepFilter train: {len(train_sf)} steps {train_sf.label_distribution()}')\n",
    "print(f'StepFilter val:   {len(val_sf)} steps {val_sf.label_distribution()}')\n",
    "\n",
    "train_as = AnchorSelectionDataset(train_recs, fe)\n",
    "val_as = AnchorSelectionDataset(val_recs, fe)\n",
    "print(f'\\nAnchorSelector train: {len(train_as)} samples {train_as.label_distribution()}')\n",
    "print(f'AnchorSelector val:   {len(val_as)} samples {val_as.label_distribution()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.training.train import train_step_filter, train_anchor_selector\n",
    "\n",
    "# Train Step Filter\n",
    "sf_results = train_step_filter(\n",
    "    train_recs, val_recs,\n",
    "    lr=1e-3, batch_size=32, epochs=100, patience=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Anchor Selector\n",
    "as_results = train_anchor_selector(\n",
    "    train_recs, val_recs,\n",
    "    lr=1e-3, batch_size=32, epochs=100, patience=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load histories\n",
    "sf_hist_path = 'ml/checkpoints/step_filter_history.json'\n",
    "as_hist_path = 'ml/checkpoints/anchor_selector_history.json'\n",
    "\n",
    "if os.path.exists(sf_hist_path):\n",
    "    with open(sf_hist_path) as f:\n",
    "        sf_history = json.load(f)\n",
    "    epochs = [h['epoch'] for h in sf_history]\n",
    "    train_loss = [h['train_loss'] for h in sf_history]\n",
    "    val_loss = [h['val_loss'] for h in sf_history]\n",
    "    f1_scores = [h['f1'] for h in sf_history]\n",
    "    \n",
    "    print('StepFilter Training Summary:')\n",
    "    print(f'  Best val_loss: {min(val_loss):.4f} at epoch {epochs[val_loss.index(min(val_loss))]}')\n",
    "    print(f'  Best F1:       {max(f1_scores):.4f} at epoch {epochs[f1_scores.index(max(f1_scores))]}')\n",
    "\n",
    "if os.path.exists(as_hist_path):\n",
    "    with open(as_hist_path) as f:\n",
    "        as_history = json.load(f)\n",
    "    epochs = [h['epoch'] for h in as_history]\n",
    "    top1 = [h['top1_accuracy'] for h in as_history]\n",
    "    \n",
    "    print('\\nAnchorSelector Training Summary:')\n",
    "    print(f'  Best top-1 accuracy: {max(top1):.4f} at epoch {epochs[top1.index(max(top1))]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Inference on a New Route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml.inference.pipeline import create_pipeline\n",
    "\n",
    "pipeline = create_pipeline()\n",
    "\n",
    "if records:\n",
    "    test_route = records[0]\n",
    "    result = pipeline.predict(test_route)\n",
    "    \n",
    "    print('Step Decisions:')\n",
    "    for d in result['all_step_decisions']:\n",
    "        status = 'KEEP' if d['keep'] else 'DELETE'\n",
    "        print(f\"  Step {d['step_number']:2d} [{d['action']:<12}] -> {status} (p={d['keep_prob']:.3f})\")\n",
    "    \n",
    "    print(f\"\\nFiltered: {len(result['filtered_steps'])} / {len(test_route['steps'])} steps kept\")\n",
    "    \n",
    "    print('\\nAnchor Decisions:')\n",
    "    for d in result['anchor_decisions']:\n",
    "        print(f\"  Step {d['step_number']:2d} -> Candidate {d['selected_idx']} \"\n",
    "              f\"{d['selected_anchor']} (proba={d['proba']})\")\n",
    "else:\n",
    "    print('No data available for testing.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Integration Example\n",
    "\n",
    "How to integrate with the Flask app (viewer_app.py):\n",
    "\n",
    "```python\n",
    "from ml.inference.pipeline import create_pipeline\n",
    "\n",
    "# At app startup:\n",
    "ml_pipeline = create_pipeline()\n",
    "\n",
    "# After route generation in /api/route endpoint:\n",
    "route_data = {\n",
    "    'steps': route_info.get('steps', []),\n",
    "    'turns': route_info.get('turns', []),\n",
    "    'path_points': route_info.get('path_points', []),\n",
    "    'total_distance': route_info['summary']['total_distance_meters'],\n",
    "}\n",
    "decisions = ml_pipeline.predict(route_data)\n",
    "\n",
    "# Use decisions['filtered_steps'] instead of all steps\n",
    "# Use decisions['anchor_decisions'] for anchor selection\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
